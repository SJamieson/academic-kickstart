@phdthesis{Jamieson2018,
abstract = {The concept of robust vision is explored as a means to improve autonomous vehicle performance and safety. This research is applicable to both the University of Toronto's self-driving car team, aUToronto, as well as to manufacturers of autonomous road vehicles, who have been criticized for the failures of their vehicles that resulted in injuries and fatalities. The requirements of a robust vision system are identified; chiefly, it must be capable of uncertainty quantification, so this field is introduced and explored with respect to its applications in vision. With this foundation, the most commonly used computer vision algorithms are evaluated for robustness. Some experiments are performed using one of the most robust algorithms identified (Bayesian Neural Networks), on autonomous driving applications to demonstrate the advantages of uncertainty quantification. Noting that a major factor in the lack of usage of robust vision systems in autonomous driving is the computational cost, a proposal is made to use FPGAs to eliminate this relative disadvantage of Bayesian Neural Networks over the current most popular models. If future tests to validate the proposal are successful, this may pave the way for more robust vision systems to be adopted by autonomous vehicle manufacturers.},
author = {Jamieson, Stewart},
school = {University of Toronto},
title = {{Deep Learning for Robust Vision in Realtime Autonomous Driving}},
year = {2018}
}
@inproceedings{Girdhar2019_ICRA,
abstract = {This paper proposes a bandwidth tunable technique for real-time probabilistic scene modeling and mapping to enable co-robotic exploration in communication constrained environments such as the deep sea. The parameters of the system enable the user to characterize the scene complexity represented by the map, which in turn determines the bandwidth requirements. The approach is demonstrated using an underwater robot that learns an unsupervised scene model of the environment and then uses this scene model to communicate the spatial distribution of various high-level semantic scene constructs to a human operator. Preliminary experiments in an artificially constructed tank environment as well as simulated missions over a 10m√ó10m coral reef using real data show the tunability of the maps to different bandwidth constraints and science interests. To our knowledge this is the first paper to quantify how the free parameters of the unsupervised scene model impact both the scientific utility of and bandwidth required to communicate the resulting scene model.},
address = {Montreal, Canada},
author = {Girdhar, Yogesh and Cai, Levi and Jamieson, Stewart and McGuire, Nathan and Flaspohler, Genevieve and Suman, Stefano and Claus, Brian},
booktitle = {IEEE Int. Conf. Robot. Autom.},
file = {:home/stewart/sync/Literature/IEEE International Conference on Robotics and Automation/Girdhar et al. - 2019 - Streaming Scene Maps for Co-Robotic Exploration in Bandwidth Limited Environments.pdf:pdf},
title = {{Streaming Scene Maps for Co-Robotic Exploration in Bandwidth Limited Environments}},
url = {https://arxiv.org/abs/1903.03214},
year = {2019}
}
